test compile precise-output
set enable_simd
target x86_64 has_avx

function %f32_add(f32, f32) -> f32 {
block0(v0: f32, v1: f32):
  v2 = fadd v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vaddss xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vaddss xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64_add(f64, f64) -> f64 {
block0(v0: f64, v1: f64):
  v2 = fadd v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vaddsd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vaddsd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f32_sub(f32, f32) -> f32 {
block0(v0: f32, v1: f32):
  v2 = fsub v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vsubss xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vsubss xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64_sub(f64, f64) -> f64 {
block0(v0: f64, v1: f64):
  v2 = fsub v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vsubsd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vsubsd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f32_mul(f32, f32) -> f32 {
block0(v0: f32, v1: f32):
  v2 = fmul v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vmulss xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vmulss xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64_mul(f64, f64) -> f64 {
block0(v0: f64, v1: f64):
  v2 = fmul v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vmulsd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vmulsd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f32_div(f32, f32) -> f32 {
block0(v0: f32, v1: f32):
  v2 = fdiv v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vdivss xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vdivss xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64_div(f64, f64) -> f64 {
block0(v0: f64, v1: f64):
  v2 = fdiv v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vdivsd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vdivsd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f32_min(f32, f32) -> f32 {
block0(v0: f32, v1: f32):
  v2 = fmin_pseudo v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vminss xmm0, xmm1, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vminss xmm0, xmm1, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64_min(f64, f64) -> f64 {
block0(v0: f64, v1: f64):
  v2 = fmin_pseudo v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vminsd xmm0, xmm1, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vminsd xmm0, xmm1, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret

function %f32_max(f32, f32) -> f32 {
block0(v0: f32, v1: f32):
  v2 = fmax_pseudo v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vmaxss xmm0, xmm1, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vmaxss xmm0, xmm1, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64_max(f64, f64) -> f64 {
block0(v0: f64, v1: f64):
  v2 = fmax_pseudo v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vmaxsd xmm0, xmm1, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vmaxsd xmm0, xmm1, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret

function %f32x4_sqrt(f32x4) -> f32x4 {
block0(v0: f32x4):
  v1 = sqrt v0
  return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vsqrtps xmm0, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vsqrtps xmm0, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64x2_sqrt(f64x2) -> f64x2 {
block0(v0: f64x2):
  v1 = sqrt v0
  return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vsqrtpd xmm0, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vsqrtpd xmm0, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret

function %f32x4_floor(f32x4) -> f32x4 {
block0(v0: f32x4):
  v1 = floor v0
  return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vroundps xmm0, xmm0, 1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vroundps xmm0, xmm0, 1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64x2_floor(f64x2) -> f64x2 {
block0(v0: f64x2):
  v1 = floor v0
  return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vroundpd xmm0, xmm0, 1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vroundpd xmm0, xmm0, 1
;   mov rsp, rbp
;   pop rbp
;   ret

function %fcvt_low_from_sint(i32x4) -> f64x2 {
block0(v0: i32x4):
  v1 = fcvt_low_from_sint.f64x2 v0
  return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vcvtdq2pd xmm0, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vcvtdq2pd xmm0, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret

function %fcvt_from_uint(i32x4) -> f32x4 {
block0(v0: i32x4):
  v1 = fcvt_from_uint.f32x4 v0
  return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpslld xmm2, xmm0, $16
;   vpsrld xmm4, xmm2, $16
;   vpsubd xmm6, xmm0, xmm4
;   vcvtdq2ps xmm8, xmm4
;   vpsrld xmm10, xmm6, $1
;   vcvtdq2ps xmm12, xmm10
;   vaddps xmm14, xmm12, xmm12
;   vaddps xmm0, xmm14, xmm8
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpslld xmm2, xmm0, 0x10
;   vpsrld xmm4, xmm2, 0x10
;   vpsubd xmm6, xmm0, xmm4
;   vcvtdq2ps xmm8, xmm4
;   vpsrld xmm10, xmm6, 1
;   vcvtdq2ps xmm12, xmm10
;   vaddps xmm14, xmm12, xmm12
;   vaddps xmm0, xmm14, xmm8
;   mov rsp, rbp
;   pop rbp
;   ret

function %fvdemote(f64x2) -> f32x4 {
block0(v0: f64x2):
  v1 = fvdemote v0
  return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vcvtpd2ps xmm0, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vcvtpd2ps xmm0, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret

function %fvpromote_low(f32x4) -> f64x2 {
block0(v0: f32x4):
  v1 = fvpromote_low v0
  return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vcvtps2pd xmm0, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vcvtps2pd xmm0, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret

function %fcvt_to_sint_sat(f32x4) -> i32x4 {
block0(v0: f32x4):
  v1 = fcvt_to_sint_sat.i32x4 v0
  return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vcmpps xmm2, xmm0, xmm0, 0x0
;   vandps xmm4, xmm0, xmm2
;   vpxor xmm6, xmm2, xmm4
;   vcvttps2dq xmm8, xmm4
;   vpand xmm10, xmm8, xmm6
;   vpsrad xmm12, xmm10, $31
;   vpxor xmm0, xmm12, xmm8
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vcmpeqps xmm2, xmm0, xmm0
;   vandps xmm4, xmm0, xmm2
;   vpxor xmm6, xmm2, xmm4
;   vcvttps2dq xmm8, xmm4
;   vpand xmm10, xmm8, xmm6
;   vpsrad xmm12, xmm10, 0x1f
;   vpxor xmm0, xmm12, xmm8
;   mov rsp, rbp
;   pop rbp
;   ret

function %fcvt_to_sint_sat_snarrow(f64x2) -> i32x4 {
block0(v0: f64x2):
  v1 = fcvt_to_sint_sat.i64x2 v0
  v2 = vconst.i64x2 0x00
  v3 = snarrow v1, v2
  return v3
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vcmppd xmm2, xmm0, xmm0, 0x0
;   vandps xmm4, xmm2, const(0)
;   vminpd xmm6, xmm0, xmm4
;   vcvttpd2dq xmm0, xmm6
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vcmpeqpd xmm2, xmm0, xmm0
;   vandps xmm4, xmm2, xmmword ptr [rip + 0xf]
;   vminpd xmm6, xmm0, xmm4
;   vcvttpd2dq xmm0, xmm6
;   mov rsp, rbp
;   pop rbp
;   ret
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   sar bh, 0xff

function %load_and_store_f32(i64, i64) {
block0(v0: i64, v1: i64):
  v2 = load.f32 v0
  store v2, v1
  return
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vmovss xmm3, xmmword ptr [rdi + 0x0]
;   vmovss xmmword ptr [rsi + 0x0], xmm3
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vmovss xmm3, dword ptr [rdi] ; trap: heap_oob
;   vmovss dword ptr [rsi], xmm3 ; trap: heap_oob
;   mov rsp, rbp
;   pop rbp
;   ret

function %load_and_store_f64(i64, i64) {
block0(v0: i64, v1: i64):
  v2 = load.f64 v0
  store v2, v1
  return
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vmovsd xmm3, xmmword ptr [rdi + 0x0]
;   vmovsd xmmword ptr [rsi + 0x0], xmm3
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vmovsd xmm3, qword ptr [rdi] ; trap: heap_oob
;   vmovsd qword ptr [rsi], xmm3 ; trap: heap_oob
;   mov rsp, rbp
;   pop rbp
;   ret

