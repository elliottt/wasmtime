test compile precise-output
set enable_simd
target x86_64 has_avx

function %sload8x8(i64) -> i16x8 {
block0(v0: i64):
  v1 = sload8x8 v0
  return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpmovsxbw xmm0, xmmword ptr [rdi + 0x0]
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpmovsxbw xmm0, qword ptr [rdi] ; trap: heap_oob
;   mov rsp, rbp
;   pop rbp
;   ret

function %uload8x8(i64) -> i16x8 {
block0(v0: i64):
  v1 = uload8x8 v0
  return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpmovzxbw xmm0, xmmword ptr [rdi + 0x0]
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpmovzxbw xmm0, qword ptr [rdi] ; trap: heap_oob
;   mov rsp, rbp
;   pop rbp
;   ret

function %sload16x4(i64) -> i32x4 {
block0(v0: i64):
  v1 = sload16x4 v0
  return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpmovsxwd xmm0, xmmword ptr [rdi + 0x0]
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpmovsxwd xmm0, qword ptr [rdi] ; trap: heap_oob
;   mov rsp, rbp
;   pop rbp
;   ret

function %uload16x4(i64) -> i32x4 {
block0(v0: i64):
  v1 = uload16x4 v0
  return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpmovzxwd xmm0, xmmword ptr [rdi + 0x0]
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpmovzxwd xmm0, qword ptr [rdi] ; trap: heap_oob
;   mov rsp, rbp
;   pop rbp
;   ret

function %sload32x2(i64) -> i64x2 {
block0(v0: i64):
  v1 = sload32x2 v0
  return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpmovsxdq xmm0, xmmword ptr [rdi + 0x0]
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpmovsxdq xmm0, qword ptr [rdi] ; trap: heap_oob
;   mov rsp, rbp
;   pop rbp
;   ret

function %uload32x2(i64) -> i64x2 {
block0(v0: i64):
  v1 = uload32x2 v0
  return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpmovzxdq xmm0, xmmword ptr [rdi + 0x0]
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpmovzxdq xmm0, qword ptr [rdi] ; trap: heap_oob
;   mov rsp, rbp
;   pop rbp
;   ret

function %load_store_i8x16(i64, i64) {
block0(v0: i64, v1: i64):
  v2 = load.i8x16 v0
  store v2, v1
  return
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vmovdqu xmm3, xmmword ptr [rdi + 0x0]
;   vmovdqu xmmword ptr [rsi + 0x0], xmm3
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vmovdqu xmm3, xmmword ptr [rdi] ; trap: heap_oob
;   vmovdqu xmmword ptr [rsi], xmm3 ; trap: heap_oob
;   mov rsp, rbp
;   pop rbp
;   ret

function %load_store_f32x4(i64, i64) {
block0(v0: i64, v1: i64):
  v2 = load.f32x4 v0
  store v2, v1
  return
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vmovups xmm3, xmmword ptr [rdi + 0x0]
;   vmovups xmmword ptr [rsi + 0x0], xmm3
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vmovups xmm3, xmmword ptr [rdi] ; trap: heap_oob
;   vmovups xmmword ptr [rsi], xmm3 ; trap: heap_oob
;   mov rsp, rbp
;   pop rbp
;   ret

function %load_store_f64x2(i64, i64) {
block0(v0: i64, v1: i64):
  v2 = load.f64x2 v0
  store v2, v1
  return
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vmovupd xmm3, xmmword ptr [rdi + 0x0]
;   vmovupd xmmword ptr [rsi + 0x0], xmm3
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vmovupd xmm3, xmmword ptr [rdi] ; trap: heap_oob
;   vmovupd xmmword ptr [rsi], xmm3 ; trap: heap_oob
;   mov rsp, rbp
;   pop rbp
;   ret

