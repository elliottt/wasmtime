test compile precise-output
set enable_simd
target x86_64 has_avx

function %i8x16_add(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
  v2 = iadd v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpaddb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpaddb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_add(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = iadd v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpaddw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpaddw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i32x4_add(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
  v2 = iadd v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpaddd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpaddd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i64x2_add(i64x2, i64x2) -> i64x2 {
block0(v0: i64x2, v1: i64x2):
  v2 = iadd v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpaddq xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpaddq xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i8x16_add_sat(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
  v2 = sadd_sat v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpaddsb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpaddsb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_add_sat(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = sadd_sat v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpaddsw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpaddsw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %u8x16_add_sat(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
  v2 = uadd_sat v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpaddusb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpaddusb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %u16x8_add_sat(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = uadd_sat v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpaddusw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpaddusw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i8x16_sub(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
  v2 = isub v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpsubb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpsubb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_sub(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = isub v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpsubw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpsubw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i32x4_sub(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
  v2 = isub v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpsubd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpsubd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i64x2_sub(i64x2, i64x2) -> i64x2 {
block0(v0: i64x2, v1: i64x2):
  v2 = isub v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpsubq xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpsubq xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i8x16_sub_sat(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
  v2 = ssub_sat v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpsubsb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpsubsb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_sub_sat(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = ssub_sat v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpsubsw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpsubsw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %u8x16_sub_sat(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
  v2 = usub_sat v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpsubusb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpsubusb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %u16x8_sub_sat(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = usub_sat v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpsubusw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpsubusw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i8x16_avg(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
  v2 = avg_round v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpavgb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpavgb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_avg(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = avg_round v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpavgw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpavgw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_mul(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = imul v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpmullw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpmullw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i32x4_mul(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
  v2 = imul v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpmulld xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpmulld xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i32x4_extmul_high_i16x8_s(i16x8, i16x8) -> i32x4 {
block0(v0: i16x8, v1: i16x8):
  v2 = swiden_high v0
  v3 = swiden_high v1
  v4 = imul v2, v3
  return v4
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpmullw xmm3, xmm0, xmm1
;   vpmulhw xmm5, xmm0, xmm1
;   vpunpckhwd xmm0, xmm3, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpmullw xmm3, xmm0, xmm1
;   vpmulhw xmm5, xmm0, xmm1
;   vpunpckhwd xmm0, xmm3, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret

function %i32x4_extmul_low_i16x8_u(i16x8, i16x8) -> i32x4 {
block0(v0: i16x8, v1: i16x8):
  v2 = uwiden_low v0
  v3 = uwiden_low v1
  v4 = imul v2, v3
  return v4
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpmullw xmm3, xmm0, xmm1
;   vpmulhuw xmm5, xmm0, xmm1
;   vpunpcklwd xmm0, xmm3, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpmullw xmm3, xmm0, xmm1
;   vpmulhuw xmm5, xmm0, xmm1
;   vpunpcklwd xmm0, xmm3, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_sqmul_round_sat(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = sqmul_round_sat v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpmulhrsw xmm3, xmm0, xmm1
;   vpcmpeqw xmm5, xmm3, const(0)
;   vpxor xmm0, xmm3, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpmulhrsw xmm3, xmm0, xmm1
;   vpcmpeqw xmm5, xmm3, xmmword ptr [rip + 0xf]
;   vpxor xmm0, xmm3, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax - 0x7fff8000], al
;   add byte ptr [rax - 0x7fff8000], al

function %i64x2_extmul_high_i32x4_s(i32x4, i32x4) -> i64x2 {
block0(v0: i32x4, v1: i32x4):
  v2 = swiden_high v0
  v3 = swiden_high v1
  v4 = imul v2, v3
  return v4
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpshufd xmm3, xmm0, 250
;   vpshufd xmm5, xmm1, 250
;   vpmuldq xmm0, xmm3, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpshufd xmm3, xmm0, 0xfa
;   vpshufd xmm5, xmm1, 0xfa
;   vpmuldq xmm0, xmm3, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret

function %i64x2_extmul_low_i32x4_u(i32x4, i32x4) -> i64x2 {
block0(v0: i32x4, v1: i32x4):
  v2 = uwiden_low v0
  v3 = uwiden_low v1
  v4 = imul v2, v3
  return v4
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpshufd xmm3, xmm0, 80
;   vpshufd xmm5, xmm1, 80
;   vpmuludq xmm0, xmm3, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpshufd xmm3, xmm0, 0x50
;   vpshufd xmm5, xmm1, 0x50
;   vpmuludq xmm0, xmm3, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64x2_from_i32x4(i32x4) -> f64x2 {
block0(v0: i32x4):
  v1 = uwiden_low v0
  v2 = fcvt_from_uint.f64x2 v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vunpcklps xmm2, xmm0, const(0)
;   vsubpd xmm0, xmm2, const(1)
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vunpcklps xmm2, xmm0, xmmword ptr [rip + 0x14]
;   vsubpd xmm0, xmm2, xmmword ptr [rip + 0x1c]
;   mov rsp, rbp
;   pop rbp
;   ret
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], dh
;   add byte ptr [r8], al
;   xor byte ptr [rbx], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], dh
;   add byte ptr [r8], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al

function %f32x4_add(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fadd v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vaddps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vaddps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64x2_add(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fadd v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vaddpd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vaddpd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f32x4_sub(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fsub v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vsubps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vsubps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64x2_sub(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fsub v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vsubpd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vsubpd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f32x4_mul(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fmul v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vmulps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vmulps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64x2_mul(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fmul v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vmulpd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vmulpd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f32x4_div(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fdiv v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vdivps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vdivps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64x2_div(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fdiv v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vdivpd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vdivpd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i8x16_ishr(i8x16, i32) -> i8x16 {
block0(v0: i8x16, v1: i32):
  v2 = sshr v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov r9, rdi
;   and r9, r9, $7
;   vpunpcklbw xmm5, xmm0, xmm0
;   vpunpckhbw xmm7, xmm0, xmm0
;   add r9d, r9d, $8
;   vmovd xmm11, r9d
;   vpsraw xmm13, xmm5, xmm11
;   vpsraw xmm15, xmm7, xmm11
;   vpacksswb xmm0, xmm13, xmm15
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov r9, rdi
;   and r9, 7
;   vpunpcklbw xmm5, xmm0, xmm0
;   vpunpckhbw xmm7, xmm0, xmm0
;   add r9d, 8
;   vmovd xmm11, r9d
;   vpsraw xmm13, xmm5, xmm11
;   vpsraw xmm15, xmm7, xmm11
;   vpacksswb xmm0, xmm13, xmm15
;   mov rsp, rbp
;   pop rbp
;   ret

function %i8x16_ishr_imm(i8x16) -> i8x16 {
block0(v0: i8x16):
  v1 = iconst.i32 3
  v2 = sshr v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpunpcklbw xmm2, xmm0, xmm0
;   vpunpckhbw xmm4, xmm0, xmm0
;   vpsraw xmm6, xmm2, $11
;   vpsraw xmm8, xmm4, $11
;   vpacksswb xmm0, xmm6, xmm8
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpunpcklbw xmm2, xmm0, xmm0
;   vpunpckhbw xmm4, xmm0, xmm0
;   vpsraw xmm6, xmm2, 0xb
;   vpsraw xmm8, xmm4, 0xb
;   vpacksswb xmm0, xmm6, xmm8
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_ishr(i16x8, i32) -> i16x8 {
block0(v0: i16x8, v1: i32):
  v2 = sshr v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rdi
;   and rcx, rcx, $15
;   vmovd xmm5, ecx
;   vpsraw xmm0, xmm0, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rdi
;   and rcx, 0xf
;   vmovd xmm5, ecx
;   vpsraw xmm0, xmm0, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_ishr_imm(i16x8) -> i16x8 {
block0(v0: i16x8):
  v1 = iconst.i32 3
  v2 = sshr v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpsraw xmm0, xmm0, $3
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpsraw xmm0, xmm0, 3
;   mov rsp, rbp
;   pop rbp
;   ret

function %i32x4_ishr(i32x4, i32) -> i32x4 {
block0(v0: i32x4, v1: i32):
  v2 = sshr v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rdi
;   and rcx, rcx, $31
;   vmovd xmm5, ecx
;   vpsrad xmm0, xmm0, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rdi
;   and rcx, 0x1f
;   vmovd xmm5, ecx
;   vpsrad xmm0, xmm0, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret

function %i32x4_ishr_imm(i32x4) -> i32x4 {
block0(v0: i32x4):
  v1 = iconst.i32 3
  v2 = sshr v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpsrad xmm0, xmm0, $3
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpsrad xmm0, xmm0, 3
;   mov rsp, rbp
;   pop rbp
;   ret

function %i8x16_snarrow(i16x8, i16x8) -> i8x16 {
block0(v0: i16x8, v1: i16x8):
  v2 = snarrow v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpacksswb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpacksswb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i8x16_unarrow(i16x8, i16x8) -> i8x16 {
block0(v0: i16x8, v1: i16x8):
  v2 = unarrow v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpackuswb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpackuswb xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_snarrow(i32x4, i32x4) -> i16x8 {
block0(v0: i32x4, v1: i32x4):
  v2 = snarrow v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpackssdw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpackssdw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_unarrow(i32x4, i32x4) -> i16x8 {
block0(v0: i32x4, v1: i32x4):
  v2 = unarrow v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpackusdw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpackusdw xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i8x16_uwiden_high(i8x16) -> i16x8 {
block0(v0: i8x16):
  v1 = uwiden_high v0
  return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   uninit xmm2
;   vpxor xmm4, xmm2, xmm2
;   vpunpckhbw xmm0, xmm0, xmm4
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpxor xmm4, xmm2, xmm2
;   vpunpckhbw xmm0, xmm0, xmm4
;   mov rsp, rbp
;   pop rbp
;   ret

function %i8x16_iadd_pairwise(i8x16) -> i16x8 {
block0(v0: i8x16):
  v1 = swiden_high v0
  v2 = swiden_low v0
  v3 = iadd_pairwise v2, v1
  return v3
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vmovdqu xmm2, const(0)
;   vpmaddubsw xmm0, xmm2, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vmovdqu xmm2, xmmword ptr [rip + 0x14]
;   vpmaddubsw xmm0, xmm2, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add dword ptr [rcx], eax
;   add dword ptr [rcx], eax
;   add dword ptr [rcx], eax
;   add dword ptr [rcx], eax
;   add dword ptr [rcx], eax
;   add dword ptr [rcx], eax
;   add dword ptr [rcx], eax
;   add dword ptr [rcx], eax

function %i16x8_iadd_pairwise(i16x8) -> i32x4 {
block0(v0: i16x8):
  v1 = swiden_high v0
  v2 = swiden_low v0
  v3 = iadd_pairwise v2, v1
  return v3
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpmaddwd xmm0, xmm0, const(0)
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpmaddwd xmm0, xmm0, xmmword ptr [rip + 0x14]
;   mov rsp, rbp
;   pop rbp
;   ret
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rcx], al
;   add byte ptr [rcx], al
;   add byte ptr [rcx], al
;   add byte ptr [rcx], al
;   add byte ptr [rcx], al
;   add byte ptr [rcx], al
;   add byte ptr [rcx], al
;   add byte ptr [rcx], al

function %i8x16_splat(i8) -> i8x16 {
block0(v0: i8):
  v1 = splat.i8x16 v0
  return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vmovd xmm2, edi
;   uninit xmm4
;   vpxor xmm6, xmm4, xmm4
;   vpshufb xmm0, xmm2, xmm6
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vmovd xmm2, edi
;   vpxor xmm6, xmm4, xmm4
;   vpshufb xmm0, xmm2, xmm6
;   mov rsp, rbp
;   pop rbp
;   ret

function %i32x4_trunc_sat_f64x2_u_zero(f64x2) -> i32x4 {
block0(v0: f64x2):
  v1 = fcvt_to_uint_sat.i64x2 v0
  v2 = vconst.i64x2 0x00
  v3 = uunarrow v1, v2
  return v3
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   uninit xmm2
;   vxorpd xmm4, xmm2, xmm2
;   vmaxpd xmm6, xmm0, xmm4
;   vminpd xmm8, xmm6, const(0)
;   vroundpd xmm10, xmm8, 3
;   vaddpd xmm12, xmm10, const(1)
;   vshufps xmm0, xmm12, xmm4, 0x88
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vxorpd xmm4, xmm2, xmm2
;   vmaxpd xmm6, xmm0, xmm4
;   vminpd xmm8, xmm6, xmmword ptr [rip + 0x1c]
;   vroundpd xmm10, xmm8, 3
;   vaddpd xmm12, xmm10, xmmword ptr [rip + 0x1e]
;   vshufps xmm0, xmm12, xmm4, 0x88
;   mov rsp, rbp
;   pop rbp
;   ret
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   loopne 0x33

function %i8x16_shl(i8x16, i32) -> i8x16 {
block0(v0: i8x16, v1: i32):
    v2 = ishl v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov r10, rdi
;   and r10, r10, $7
;   vmovd xmm5, r10d
;   vpsllw xmm7, xmm0, xmm5
;   lea rsi, const(0)
;   shl r10, r10, 0x4
;   vmovdqu xmm13, xmmword ptr [rsi + r10]
;   vpand xmm0, xmm7, xmm13
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov r10, rdi
;   and r10, 7
;   vmovd xmm5, r10d
;   vpsllw xmm7, xmm0, xmm5
;   lea rsi, [rip + 0x15]
;   shl r10, 4
;   vmovdqu xmm13, xmmword ptr [rsi + r10]
;   vpand xmm0, xmm7, xmm13
;   mov rsp, rbp
;   pop rbp
;   ret
;   add bh, bh

function %i8x16_shl_imm(i8x16) -> i8x16 {
block0(v0: i8x16):
    v1 = iconst.i32 1
    v2 = ishl v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpsllw xmm2, xmm0, $1
;   vmovdqu xmm4, const(0)
;   vpand xmm0, xmm2, xmm4
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpsllw xmm2, xmm0, 1
;   vmovdqu xmm4, xmmword ptr [rip + 0xf]
;   vpand xmm0, xmm2, xmm4
;   mov rsp, rbp
;   pop rbp
;   ret
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al

function %i16x8_shl(i16x8, i32) -> i16x8 {
block0(v0: i16x8, v1: i32):
    v2 = ishl v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rdi
;   and rcx, rcx, $15
;   vmovd xmm5, ecx
;   vpsllw xmm0, xmm0, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rdi
;   and rcx, 0xf
;   vmovd xmm5, ecx
;   vpsllw xmm0, xmm0, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_shl_imm(i16x8) -> i16x8 {
block0(v0: i16x8):
    v1 = iconst.i32 1
    v2 = ishl v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpsllw xmm0, xmm0, $1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpsllw xmm0, xmm0, 1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i32x4_shl(i32x4, i32) -> i32x4 {
block0(v0: i32x4, v1: i32):
    v2 = ishl v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rdi
;   and rcx, rcx, $31
;   vmovd xmm5, ecx
;   vpslld xmm0, xmm0, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rdi
;   and rcx, 0x1f
;   vmovd xmm5, ecx
;   vpslld xmm0, xmm0, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret

function %i32x4_shl_imm(i32x4) -> i32x4 {
block0(v0: i32x4):
    v1 = iconst.i32 1
    v2 = ishl v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpslld xmm0, xmm0, $1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpslld xmm0, xmm0, 1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i64x2_shl(i64x2, i32) -> i64x2 {
block0(v0: i64x2, v1: i32):
    v2 = ishl v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rdi
;   and rcx, rcx, $63
;   vmovd xmm5, ecx
;   vpsllq xmm0, xmm0, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rdi
;   and rcx, 0x3f
;   vmovd xmm5, ecx
;   vpsllq xmm0, xmm0, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret

function %i64x2_shl_imm(i64x2) -> i64x2 {
block0(v0: i64x2):
    v1 = iconst.i32 1
    v2 = ishl v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpsllq xmm0, xmm0, $1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpsllq xmm0, xmm0, 1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i8x16_ushr(i8x16, i32) -> i8x16 {
block0(v0: i8x16, v1: i32):
    v2 = ushr v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov r10, rdi
;   and r10, r10, $7
;   vmovd xmm5, r10d
;   vpsrlw xmm7, xmm0, xmm5
;   lea rsi, const(0)
;   shl r10, r10, 0x4
;   vpand xmm0, xmm7, qword ptr [rsi + r10]
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov r10, rdi
;   and r10, 7
;   vmovd xmm5, r10d
;   vpsrlw xmm7, xmm0, xmm5
;   lea rsi, [rip + 0x15]
;   shl r10, 4
;   vpand xmm0, xmm7, xmmword ptr [rsi + r10]
;   mov rsp, rbp
;   pop rbp
;   ret
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al

function %i8x16_ushr_imm(i8x16) -> i8x16 {
block0(v0: i8x16):
    v1 = iconst.i32 1
    v2 = ushr v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpsrlw xmm2, xmm0, $1
;   vpand xmm0, xmm2, const(0)
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpsrlw xmm2, xmm0, 1
;   vpand xmm0, xmm2, xmmword ptr [rip + 0xf]
;   mov rsp, rbp
;   pop rbp
;   ret
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   jg 0xa1
;   jg 0xa3
;   jg 0xa5
;   jg 0xa7
;   jg 0xa9
;   jg 0xab
;   jg 0xad
;   jg 0xaf

function %i16x8_ushr(i16x8, i32) -> i16x8 {
block0(v0: i16x8, v1: i32):
    v2 = ushr v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rdi
;   and rcx, rcx, $15
;   vmovd xmm5, ecx
;   vpsrlw xmm0, xmm0, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rdi
;   and rcx, 0xf
;   vmovd xmm5, ecx
;   vpsrlw xmm0, xmm0, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_ushr_imm(i16x8) -> i16x8 {
block0(v0: i16x8):
    v1 = iconst.i32 1
    v2 = ushr v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpsrlw xmm0, xmm0, $1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpsrlw xmm0, xmm0, 1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i32x4_ushr(i32x4, i32) -> i32x4 {
block0(v0: i32x4, v1: i32):
    v2 = ushr v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rdi
;   and rcx, rcx, $31
;   vmovd xmm5, ecx
;   vpsrld xmm0, xmm0, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rdi
;   and rcx, 0x1f
;   vmovd xmm5, ecx
;   vpsrld xmm0, xmm0, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret

function %i32x4_ushr_imm(i32x4) -> i32x4 {
block0(v0: i32x4):
    v1 = iconst.i32 1
    v2 = ushr v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpsrld xmm0, xmm0, $1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpsrld xmm0, xmm0, 1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i64x2_ushr(i64x2, i32) -> i64x2 {
block0(v0: i64x2, v1: i32):
    v2 = ushr v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rdi
;   and rcx, rcx, $63
;   vmovd xmm5, ecx
;   vpsrlq xmm0, xmm0, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rdi
;   and rcx, 0x3f
;   vmovd xmm5, ecx
;   vpsrlq xmm0, xmm0, xmm5
;   mov rsp, rbp
;   pop rbp
;   ret

function %i64x2_ushr_imm(i64x2) -> i64x2 {
block0(v0: i64x2):
    v1 = iconst.i32 1
    v2 = ushr v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpsrlq xmm0, xmm0, $1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpsrlq xmm0, xmm0, 1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i8x16_abs(i8x16) -> i8x16 {
block0(v0: i8x16):
    v1 = iabs v0
    return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpabsb xmm0, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpabsb xmm0, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_abs(i16x8) -> i16x8 {
block0(v0: i16x8):
    v1 = iabs v0
    return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpabsw xmm0, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpabsw xmm0, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret

function %i32x4_abs(i32x4) -> i32x4 {
block0(v0: i32x4):
    v1 = iabs v0
    return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpabsd xmm0, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpabsd xmm0, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret

function %palignr_11(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = shuffle v0, v1, [11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26]
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpalignr xmm0, xmm1, xmm0, 0xb
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpalignr xmm0, xmm1, xmm0, 0xb
;   mov rsp, rbp
;   pop rbp
;   ret

