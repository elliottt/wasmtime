test compile precise-output
set enable_llvm_abi_extensions=true
target x86_64


function %sshr_i128_i128(i128, i8) -> i128 {
block0(v0: i128, v1: i8):
    v2 = uextend.i64 v1
    v3 = iconcat v2, v2

    v4 = sshr.i128 v0, v3

    return v4
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   movzx rcx, dl
;   mov r8, rdi
;   shr r8, r8, cl
;   mov r10, rsi
;   sar r10, r10, cl
;   mov r11, rcx
;   mov ecx, 0x40
;   mov rax, r11
;   sub rcx, rcx, rax
;   mov r9, rsi
;   shl r9, r9, cl
;   xor r11, r11, r11
;   test rax, $127
;   cmovz r9, r11, r9
;   or r8, r8, r9
;   mov rdx, rsi
;   sar rdx, rdx, 0x3f
;   test rax, $64
;   mov rax, r10
;   cmovz rax, r8, rax
;   cmovz rdx, r10, rdx
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   movzx rcx, dl
;   mov r8, rdi
;   shr r8, cl
;   mov r10, rsi
;   sar r10, cl
;   mov r11, rcx
;   mov ecx, 0x40
;   mov rax, r11
;   sub rcx, rax
;   mov r9, rsi
;   shl r9, cl
;   xor r11, r11
;   test rax, 0x7f
;   cmove r9, r11
;   or r8, r9
;   mov rdx, rsi
;   sar rdx, 0x3f
;   test rax, 0x40
;   mov rax, r10
;   cmove rax, r8
;   cmove rdx, r10
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i128_i64(i128, i64) -> i128 {
block0(v0: i128, v1: i64):
    v2 = sshr.i128 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rdx
;   mov r10, rdx
;   mov r11, rdi
;   shr r11, r11, cl
;   mov r9, rsi
;   sar r9, r9, cl
;   mov r10, rcx
;   mov ecx, 0x40
;   mov rdi, r10
;   sub rcx, rcx, rdi
;   mov r8, rsi
;   shl r8, r8, cl
;   xor r10, r10, r10
;   test rdi, $127
;   cmovz r8, r10, r8
;   or r11, r11, r8
;   mov rdx, rsi
;   sar rdx, rdx, 0x3f
;   test rdi, $64
;   mov rax, r9
;   cmovz rax, r11, rax
;   cmovz rdx, r9, rdx
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rdx
;   mov r10, rdx
;   mov r11, rdi
;   shr r11, cl
;   mov r9, rsi
;   sar r9, cl
;   mov r10, rcx
;   mov ecx, 0x40
;   mov rdi, r10
;   sub rcx, rdi
;   mov r8, rsi
;   shl r8, cl
;   xor r10, r10
;   test rdi, 0x7f
;   cmove r8, r10
;   or r11, r8
;   mov rdx, rsi
;   sar rdx, 0x3f
;   test rdi, 0x40
;   mov rax, r9
;   cmove rax, r11
;   cmove rdx, r9
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i128_i32(i128, i32) -> i128 {
block0(v0: i128, v1: i32):
    v2 = sshr.i128 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rdx
;   mov r10, rdx
;   mov r11, rdi
;   shr r11, r11, cl
;   mov r9, rsi
;   sar r9, r9, cl
;   mov r10, rcx
;   mov ecx, 0x40
;   mov rdi, r10
;   sub rcx, rcx, rdi
;   mov r8, rsi
;   shl r8, r8, cl
;   xor r10, r10, r10
;   test rdi, $127
;   cmovz r8, r10, r8
;   or r11, r11, r8
;   mov rdx, rsi
;   sar rdx, rdx, 0x3f
;   test rdi, $64
;   mov rax, r9
;   cmovz rax, r11, rax
;   cmovz rdx, r9, rdx
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rdx
;   mov r10, rdx
;   mov r11, rdi
;   shr r11, cl
;   mov r9, rsi
;   sar r9, cl
;   mov r10, rcx
;   mov ecx, 0x40
;   mov rdi, r10
;   sub rcx, rdi
;   mov r8, rsi
;   shl r8, cl
;   xor r10, r10
;   test rdi, 0x7f
;   cmove r8, r10
;   or r11, r8
;   mov rdx, rsi
;   sar rdx, 0x3f
;   test rdi, 0x40
;   mov rax, r9
;   cmove rax, r11
;   cmove rdx, r9
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i128_i16(i128, i16) -> i128 {
block0(v0: i128, v1: i16):
    v2 = sshr.i128 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rdx
;   mov r10, rdx
;   mov r11, rdi
;   shr r11, r11, cl
;   mov r9, rsi
;   sar r9, r9, cl
;   mov r10, rcx
;   mov ecx, 0x40
;   mov rdi, r10
;   sub rcx, rcx, rdi
;   mov r8, rsi
;   shl r8, r8, cl
;   xor r10, r10, r10
;   test rdi, $127
;   cmovz r8, r10, r8
;   or r11, r11, r8
;   mov rdx, rsi
;   sar rdx, rdx, 0x3f
;   test rdi, $64
;   mov rax, r9
;   cmovz rax, r11, rax
;   cmovz rdx, r9, rdx
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rdx
;   mov r10, rdx
;   mov r11, rdi
;   shr r11, cl
;   mov r9, rsi
;   sar r9, cl
;   mov r10, rcx
;   mov ecx, 0x40
;   mov rdi, r10
;   sub rcx, rdi
;   mov r8, rsi
;   shl r8, cl
;   xor r10, r10
;   test rdi, 0x7f
;   cmove r8, r10
;   or r11, r8
;   mov rdx, rsi
;   sar rdx, 0x3f
;   test rdi, 0x40
;   mov rax, r9
;   cmove rax, r11
;   cmove rdx, r9
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i128_i8(i128, i8) -> i128 {
block0(v0: i128, v1: i8):
    v2 = sshr.i128 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rdx
;   mov r10, rdx
;   mov r11, rdi
;   shr r11, r11, cl
;   mov r9, rsi
;   sar r9, r9, cl
;   mov r10, rcx
;   mov ecx, 0x40
;   mov rdi, r10
;   sub rcx, rcx, rdi
;   mov r8, rsi
;   shl r8, r8, cl
;   xor r10, r10, r10
;   test rdi, $127
;   cmovz r8, r10, r8
;   or r11, r11, r8
;   mov rdx, rsi
;   sar rdx, rdx, 0x3f
;   test rdi, $64
;   mov rax, r9
;   cmovz rax, r11, rax
;   cmovz rdx, r9, rdx
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rdx
;   mov r10, rdx
;   mov r11, rdi
;   shr r11, cl
;   mov r9, rsi
;   sar r9, cl
;   mov r10, rcx
;   mov ecx, 0x40
;   mov rdi, r10
;   sub rcx, rdi
;   mov r8, rsi
;   shl r8, cl
;   xor r10, r10
;   test rdi, 0x7f
;   cmove r8, r10
;   or r11, r8
;   mov rdx, rsi
;   sar rdx, 0x3f
;   test rdi, 0x40
;   mov rax, r9
;   cmove rax, r11
;   cmove rdx, r9
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i64_i128(i64, i128) -> i64 {
block0(v0: i64, v1: i128):
    v2 = sshr.i64 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   mov rax, rdi
;   sar rax, rax, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   mov rax, rdi
;   sar rax, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i32_i128(i32, i128) -> i32 {
block0(v0: i32, v1: i128):
    v2 = sshr.i32 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   mov rax, rdi
;   sar eax, eax, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   mov rax, rdi
;   sar eax, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i16_i128(i16, i128) -> i16 {
block0(v0: i16, v1: i128):
    v2 = sshr.i16 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   and rcx, rcx, $15
;   mov rax, rdi
;   sar ax, ax, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   and rcx, 0xf
;   mov rax, rdi
;   sar ax, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i8_i128(i8, i128) -> i8 {
block0(v0: i8, v1: i128):
    v2 = sshr.i8 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   and rcx, rcx, $7
;   mov rax, rdi
;   sar al, al, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   and rcx, 7
;   mov rax, rdi
;   sar al, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i64_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = sshr.i64 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   mov rax, rdi
;   sar rax, rax, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   mov rax, rdi
;   sar rax, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i64_i32(i64, i32) -> i64 {
block0(v0: i64, v1: i32):
    v2 = sshr.i64 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   mov rax, rdi
;   sar rax, rax, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   mov rax, rdi
;   sar rax, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i64_i16(i64, i16) -> i64 {
block0(v0: i64, v1: i16):
    v2 = sshr.i64 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   mov rax, rdi
;   sar rax, rax, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   mov rax, rdi
;   sar rax, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i64_i8(i64, i8) -> i64 {
block0(v0: i64, v1: i8):
    v2 = sshr.i64 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   mov rax, rdi
;   sar rax, rax, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   mov rax, rdi
;   sar rax, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i32_i64(i32, i64) -> i32 {
block0(v0: i32, v1: i64):
    v2 = sshr.i32 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   mov rax, rdi
;   sar eax, eax, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   mov rax, rdi
;   sar eax, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i32_i32(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = sshr.i32 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   mov rax, rdi
;   sar eax, eax, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   mov rax, rdi
;   sar eax, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i32_i16(i32, i16) -> i32 {
block0(v0: i32, v1: i16):
    v2 = sshr.i32 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   mov rax, rdi
;   sar eax, eax, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   mov rax, rdi
;   sar eax, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i32_i8(i32, i8) -> i32 {
block0(v0: i32, v1: i8):
    v2 = sshr.i32 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   mov rax, rdi
;   sar eax, eax, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   mov rax, rdi
;   sar eax, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i16_i64(i16, i64) -> i16 {
block0(v0: i16, v1: i64):
    v2 = sshr.i16 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   and rcx, rcx, $15
;   mov rax, rdi
;   sar ax, ax, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   and rcx, 0xf
;   mov rax, rdi
;   sar ax, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i16_i32(i16, i32) -> i16 {
block0(v0: i16, v1: i32):
    v2 = sshr.i16 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   and rcx, rcx, $15
;   mov rax, rdi
;   sar ax, ax, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   and rcx, 0xf
;   mov rax, rdi
;   sar ax, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i16_i16(i16, i16) -> i16 {
block0(v0: i16, v1: i16):
    v2 = sshr.i16 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   and rcx, rcx, $15
;   mov rax, rdi
;   sar ax, ax, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   and rcx, 0xf
;   mov rax, rdi
;   sar ax, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i16_i8(i16, i8) -> i16 {
block0(v0: i16, v1: i8):
    v2 = sshr.i16 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   and rcx, rcx, $15
;   mov rax, rdi
;   sar ax, ax, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   and rcx, 0xf
;   mov rax, rdi
;   sar ax, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i8_i64(i8, i64) -> i8 {
block0(v0: i8, v1: i64):
    v2 = sshr.i8 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   and rcx, rcx, $7
;   mov rax, rdi
;   sar al, al, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   and rcx, 7
;   mov rax, rdi
;   sar al, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i8_i32(i8, i32) -> i8 {
block0(v0: i8, v1: i32):
    v2 = sshr.i8 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   and rcx, rcx, $7
;   mov rax, rdi
;   sar al, al, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   and rcx, 7
;   mov rax, rdi
;   sar al, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i8_i16(i8, i16) -> i8 {
block0(v0: i8, v1: i16):
    v2 = sshr.i8 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   and rcx, rcx, $7
;   mov rax, rdi
;   sar al, al, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   and rcx, 7
;   mov rax, rdi
;   sar al, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i8_i8(i8, i8) -> i8 {
block0(v0: i8, v1: i8):
    v2 = sshr.i8 v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rcx, rsi
;   and rcx, rcx, $7
;   mov rax, rdi
;   sar al, al, cl
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rcx, rsi
;   and rcx, 7
;   mov rax, rdi
;   sar al, cl
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i64_const(i64) -> i64 {
block0(v0: i64):
    v1 = sshr_imm.i64 v0, 65
    return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rax, rdi
;   sar rax, rax, 0x1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rax, rdi
;   sar rax, 1
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i32_const(i32) -> i32 {
block0(v0: i32):
    v1 = sshr_imm.i32 v0, 33
    return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rax, rdi
;   sar eax, eax, 0x1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rax, rdi
;   sar eax, 1
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i16_const(i16) -> i16 {
block0(v0: i16):
    v1 = sshr_imm.i16 v0, 17
    return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rax, rdi
;   sar ax, ax, 0x1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rax, rdi
;   sar ax, 1
;   mov rsp, rbp
;   pop rbp
;   ret

function %sshr_i8_const(i8) -> i8 {
block0(v0: i8):
    v1 = sshr_imm.i8 v0, 9
    return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov rax, rdi
;   sar al, al, 0x1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov rax, rdi
;   sar al, 1
;   mov rsp, rbp
;   pop rbp
;   ret

