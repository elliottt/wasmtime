test compile precise-output
set enable_simd
target x86_64 has_avx

function %or_from_memory(f32x4, i64) -> f32x4 {
block0(v0: f32x4, v1: i64):
    v2 = load.f32x4 notrap aligned v1
    v3 = bor v0, v2
    return v3
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vorps xmm0, xmm0, qword ptr [rdi + 0x0]
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vorps xmm0, xmm0, xmmword ptr [rdi]
;   mov rsp, rbp
;   pop rbp
;   ret

function %copysign_from_memory(i64) -> f32 {
block0(v0: i64):
    v1 = f32const 0.0
    v2 = load.f32 notrap aligned v0
    v3 = fcopysign v1, v2
    return v3
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   mov eax, 0x80000000
;   vmovd xmm4, eax
;   vandnps xmm6, xmm4, const(0)
;   vandps xmm8, xmm4, qword ptr [rdi + 0x0]
;   vorps xmm0, xmm6, xmm8
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   mov eax, 0x80000000
;   vmovd xmm4, eax
;   vandnps xmm6, xmm4, xmmword ptr [rip + 0x1b]
;   vandps xmm8, xmm4, xmmword ptr [rdi]
;   vorps xmm0, xmm6, xmm8
;   mov rsp, rbp
;   pop rbp
;   ret
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al
;   add byte ptr [rax], al

function %bor_f32x4(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
    v2 = bor v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vorps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vorps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %band_not_f32x4(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
    v2 = band_not v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vandnps xmm0, xmm1, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vandnps xmm0, xmm1, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret

function %band_not_f64x2(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
    v2 = band_not v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vandnpd xmm0, xmm1, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vandnpd xmm0, xmm1, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret

function %band_not_i64x2(i64x2, i64x2) -> i64x2 {
block0(v0: i64x2, v1: i64x2):
    v2 = band_not v0, v1
    return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpandn xmm0, xmm1, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpandn xmm0, xmm1, xmm0
;   mov rsp, rbp
;   pop rbp
;   ret

function %f32x4_abs(f32x4) -> f32x4 {
block0(v0: f32x4):
    v1 = fabs v0
    return v1
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   uninit xmm2
;   vpcmpeqd xmm4, xmm2, xmm2
;   vpsrld xmm6, xmm4, $1
;   vandps xmm0, xmm0, xmm6
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpcmpeqd xmm4, xmm2, xmm2
;   vpsrld xmm6, xmm4, 1
;   vandps xmm0, xmm0, xmm6
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_and(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = band v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpand xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpand xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f32x4_and(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = band v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vandps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vandps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64x2_and(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = band v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vandpd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vandpd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_or(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = bor v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpor xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpor xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f32x4_or(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = bor v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vorps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vorps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64x2_or(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = bor v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vorpd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vorpd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_xor(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = bxor v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpxor xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpxor xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f32x4_xor(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = bxor v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vxorps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vxorps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64x2_xor(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = bxor v0, v1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vxorpd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vxorpd xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_bitselect(i16x8, i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8, v2: i16x8):
  v3 = bitselect v0, v1, v2
  return v3
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpand xmm4, xmm1, xmm0
;   vpandn xmm6, xmm0, xmm2
;   vpor xmm0, xmm6, xmm4
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpand xmm4, xmm1, xmm0
;   vpandn xmm6, xmm0, xmm2
;   vpor xmm0, xmm6, xmm4
;   mov rsp, rbp
;   pop rbp
;   ret

function %f32x4_bitselect(f32x4, f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4, v2: f32x4):
  v3 = bitselect v0, v1, v2
  return v3
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vandps xmm4, xmm1, xmm0
;   vandnps xmm6, xmm0, xmm2
;   vorps xmm0, xmm6, xmm4
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vandps xmm4, xmm1, xmm0
;   vandnps xmm6, xmm0, xmm2
;   vorps xmm0, xmm6, xmm4
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64x2_bitselect(f64x2, f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2, v2: f64x2):
  v3 = bitselect v0, v1, v2
  return v3
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vandpd xmm4, xmm1, xmm0
;   vandnpd xmm6, xmm0, xmm2
;   vorpd xmm0, xmm6, xmm4
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vandpd xmm4, xmm1, xmm0
;   vandnpd xmm6, xmm0, xmm2
;   vorpd xmm0, xmm6, xmm4
;   mov rsp, rbp
;   pop rbp
;   ret

function %f32x4_replace_lane(f32x4, f32) -> f32x4 {
block0(v0: f32x4, v1: f32):
  v2 = insertlane v0, v1, 1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vinsertps xmm0, xmm0, xmm1, 0x10
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vinsertps xmm0, xmm0, xmm1, 0x10
;   mov rsp, rbp
;   pop rbp
;   ret

function %f64x2_replace_lane(f64x2, f64) -> f64x2 {
block0(v0: f64x2, v1: f64):
  v2 = insertlane v0, v1, 1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vmovlhps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vmovlhps xmm0, xmm0, xmm1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i8x16_replace_lane(i8x16, i8) -> i8x16 {
block0(v0: i8x16, v1: i8):
  v2 = insertlane v0, v1, 1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpinsrb xmm0, xmm0, rdi, 0x1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpinsrb xmm0, xmm0, edi, 1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i16x8_replace_lane(i16x8, i16) -> i16x8 {
block0(v0: i16x8, v1: i16):
  v2 = insertlane v0, v1, 1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpinsrw xmm0, xmm0, rdi, 0x1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpinsrw xmm0, xmm0, edi, 1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i32x4_replace_lane(i32x4, i32) -> i32x4 {
block0(v0: i32x4, v1: i32):
  v2 = insertlane v0, v1, 1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpinsrd xmm0, xmm0, rdi, 0x1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpinsrd xmm0, xmm0, edi, 1
;   mov rsp, rbp
;   pop rbp
;   ret

function %i64x2_replace_lane(i64x2, i64) -> i64x2 {
block0(v0: i64x2, v1: i64):
  v2 = insertlane v0, v1, 1
  return v2
}

; VCode:
;   push rbp
;   mov rbp, rsp
; block0:
;   vpinsrq xmm0, xmm0, rdi, 0x1
;   mov rsp, rbp
;   pop rbp
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   push rbp
;   mov rbp, rsp
; block1: ; offset 0x4
;   vpinsrq xmm0, xmm0, rdi, 1
;   mov rsp, rbp
;   pop rbp
;   ret

